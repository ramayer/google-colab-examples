{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHPDjI0sXkmE44GMzmvX1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramayer/google-colab-examples/blob/main/Efficient_spark_range_joins_in_Databricks_vs_Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Efficient spark range joins in Databricks vs Apache Spark\n",
        "\n",
        "## Databricks spark has efficient range joins through hints.\n",
        "\n",
        "https://docs.databricks.com/en/optimizations/range-join.html\n",
        "\n",
        "\n",
        "## Apache Spark seems not to, yet.\n",
        "\n",
        "* https://github.com/apache/spark/pull/7379\n",
        "* https://issues.apache.org/jira/browse/SPARK-8682\n",
        "\n",
        "Zach Moshe describes a workaround.\n",
        "\n",
        "* http://zachmoshe.com/2016/09/26/efficient-range-joins-with-spark.html\n",
        "\n",
        "This notebook implements something simlar to each of the above.\n"
      ],
      "metadata": {
        "id": "BZs6cHBij5rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmn5B1KWWNBb",
        "outputId": "1c96a8df-0e23-4617-c23d-501f028b2cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import pyspark, findspark, delta\n",
        "except:\n",
        "   %pip install -q --upgrade pyspark==3.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "MAX_MEMORY=\"8g\"\n",
        "maven_coords = [\n",
        "    'io.delta:delta-spark_2.12:3.2.0',\n",
        "]\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
        "    .config(\"spark.jars.packages\", \",\".join(maven_coords))\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY)\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
        "    .enableHiveSupport()\n",
        "    .getOrCreate()\n",
        "    )\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "0jcFcg-IWn0k",
        "outputId": "19f1cd4e-e41e-432e-b8fa-f487973461d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cd1e42977f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9ef87eefe892:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.range(1000 * 1000).createOrReplaceTempView(\"a_million_rows\")"
      ],
      "metadata": {
        "id": "ClFoBZrRWkOs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.sql(\"\"\"\n",
        "  select\n",
        "    cast('2024-01-01' as timestamp) + interval '1 second' * id /4 as dttm,\n",
        "    sin(id/60 / 4) as sin,\n",
        "    cos(id/60 / 4) as cos\n",
        "  from a_million_rows\n",
        "  \"\"\")\n",
        "df1.write.format(\"delta\").mode('overwrite').saveAsTable(\"df1\")\n",
        "df1.sort('dttm').limit(3).pandas_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "cFd42NeUWkSW",
        "outputId": "1c42f678-05eb-41da-f97f-a14237603ebc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     dttm       sin       cos\n",
              "0 2024-01-01 00:00:00.000  0.000000  1.000000\n",
              "1 2024-01-01 00:00:00.250  0.004167  0.999991\n",
              "2 2024-01-01 00:00:00.500  0.008333  0.999965"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dttm</th>\n",
              "      <th>sin</th>\n",
              "      <th>cos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-01-01 00:00:00.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:00.250</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.999991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:00.500</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.999965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.sql(\"\"\"\n",
        "  select\n",
        "    cast('2024-01-01' as timestamp) + interval '1 second' * id as dttm,\n",
        "    cos(id/60) as val\n",
        "  from a_million_rows\n",
        "  \"\"\")\n",
        "df2.write.format(\"delta\").mode('overwrite').saveAsTable(\"df2\")\n",
        "df2.sort('dttm').limit(3).pandas_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OHsVK6SgZmqg",
        "outputId": "890bc601-c65a-4d30-9e6f-f76624b60219"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 dttm       val\n",
              "0 2024-01-01 00:00:00  1.000000\n",
              "1 2024-01-01 00:00:01  0.999861\n",
              "2 2024-01-01 00:00:02  0.999444"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dttm</th>\n",
              "      <th>val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>0.999861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.table(\"df1\")\n",
        "df2 = spark.table(\"df2\")"
      ],
      "metadata": {
        "id": "B6htjGUViVYw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp, lag\n",
        "from pyspark.sql.window import Window\n",
        "df2_ranges = (spark.sql(\"select * from df2\")\n",
        "                .withColumn(\"prev_dttm\", lag(\"dttm\").over(Window.orderBy(\"dttm\")))\n",
        "                .withColumn(\"prev_val\", lag(\"val\").over(Window.orderBy(\"dttm\")))\n",
        "                .selectExpr(\"prev_dttm\",\"prev_val\", \"dttm as next_dttm\", \"val as next_val\")\n",
        "                )\n",
        "df2_ranges.createOrReplaceTempView(\"df2_ranges\")\n",
        "df2_ranges.sort('prev_dttm').limit(3).pandas_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "x67yts5Ua5pd",
        "outputId": "5c8a0be2-9311-4508-cec9-443467a10c8f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            prev_dttm  prev_val           next_dttm  next_val\n",
              "0                 NaT       NaN 2024-01-01 00:00:00  1.000000\n",
              "1 2024-01-01 00:00:00  1.000000 2024-01-01 00:00:01  0.999861\n",
              "2 2024-01-01 00:00:01  0.999861 2024-01-01 00:00:02  0.999444"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prev_dttm</th>\n",
              "      <th>prev_val</th>\n",
              "      <th>next_dttm</th>\n",
              "      <th>next_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>0.999861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfkUYeasf9v9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't run this on F/OSS Apache Spark, it takes forever\n",
        "# CartesianProduct and/or BroadcastNestedLoopJoin\n",
        "# (depending on spark configs) are both painful ways to do joins.\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "  SELECT df1.dttm as df1_dttm,\n",
        "         df1.sin  as df1_sin,\n",
        "         df1.cos  as df1_cos,\n",
        "         df2_ranges.prev_dttm,\n",
        "         df2_ranges.next_dttm,\n",
        "         df2_ranges.prev_val,\n",
        "         df2_ranges.next_val\n",
        "  FROM df1\n",
        "  JOIN df2_ranges ON (df1.dttm >= df2_ranges.prev_dttm\n",
        "                  AND df1.dttm <= df2_ranges.next_dttm)\n",
        "\"\"\").explain()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5SfzQSacEBY",
        "outputId": "8a472b40-24c5-4ed8-a3db-cbdd01e30554"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [dttm#1522 AS df1_dttm#1519, sin#1523 AS df1_sin#1520, cos#1524 AS df1_cos#1521, prev_dttm#604, next_dttm#613, prev_val#608, next_val#614]\n",
            "   +- CartesianProduct ((dttm#1522 >= prev_dttm#604) AND (dttm#1522 <= next_dttm#613))\n",
            "      :- Filter isnotnull(dttm#1522)\n",
            "      :  +- FileScan parquet spark_catalog.default.df1[dttm#1522,sin#1523,cos#1524] Batched: true, DataFilters: [isnotnull(dttm#1522)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "      +- Project [prev_dttm#604, prev_val#608, dttm#599 AS next_dttm#613, val#600 AS next_val#614]\n",
            "         +- Filter (isnotnull(prev_dttm#604) AND isnotnull(dttm#599))\n",
            "            +- Window [lag(dttm#599, -1, null) windowspecdefinition(dttm#599 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#604, lag(val#600, -1, null) windowspecdefinition(dttm#599 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#608], [dttm#599 ASC NULLS FIRST]\n",
            "               +- Sort [dttm#599 ASC NULLS FIRST], false, 0\n",
            "                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=581]\n",
            "                     +- FileScan parquet spark_catalog.default.df2[dttm#599,val#600] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python API does no better, even with a hint\n",
        "df1.hint(\"range_join\",6).join(df2_ranges,\n",
        "                               on=[\n",
        "                                   df1.dttm >= df2_ranges.prev_dttm,\n",
        "                                   df1.dttm <= df2_ranges.next_dttm\n",
        "                               ]).explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv1deS6umzPd",
        "outputId": "b865a18f-0aea-433e-bc1e-443a9ca4c1ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- CartesianProduct ((dttm#589 >= prev_dttm#604) AND (dttm#589 <= next_dttm#613))\n",
            "   :- Filter isnotnull(dttm#589)\n",
            "   :  +- FileScan parquet spark_catalog.default.df1[dttm#589,sin#590,cos#591] Batched: true, DataFilters: [isnotnull(dttm#589)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "   +- Project [prev_dttm#604, prev_val#608, dttm#599 AS next_dttm#613, val#600 AS next_val#614]\n",
            "      +- Filter (isnotnull(prev_dttm#604) AND isnotnull(dttm#599))\n",
            "         +- Window [lag(dttm#599, -1, null) windowspecdefinition(dttm#599 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#604, lag(val#600, -1, null) windowspecdefinition(dttm#599 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#608], [dttm#599 ASC NULLS FIRST]\n",
            "            +- Sort [dttm#599 ASC NULLS FIRST], false, 0\n",
            "               +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=657]\n",
            "                  +- FileScan parquet spark_catalog.default.df2[dttm#599,val#600] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This should be fast on Databricks\n",
        "# https://docs.databricks.com/en/optimizations/range-join.html\n",
        "# but is annoyingly not on Apache Spark\n",
        "spark.sql(\"\"\"\n",
        "  SELECT  /*+ RANGE_JOIN(dttm, 10) */\n",
        "         df1.dttm as df1_dttm,\n",
        "         df1.sin  as df1_sin,\n",
        "         df1.cos  as df1_cos,\n",
        "         df2_ranges.prev_dttm,\n",
        "         df2_ranges.next_dttm,\n",
        "         df2_ranges.prev_val,\n",
        "         df2_ranges.next_val\n",
        "  FROM df1\n",
        "  JOIN df2_ranges ON (df1.dttm >= df2_ranges.prev_dttm\n",
        "                  AND df1.dttm <= df2_ranges.next_dttm)\n",
        "\"\"\").explain()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6ERU1jGgBOU",
        "outputId": "768261d5-30b7-4143-d961-e69134a7b5e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [dttm#2205 AS df1_dttm#2202, sin#2206 AS df1_sin#2203, cos#2207 AS df1_cos#2204, prev_dttm#604, next_dttm#613, prev_val#608, next_val#614]\n",
            "   +- CartesianProduct ((dttm#2205 >= prev_dttm#604) AND (dttm#2205 <= next_dttm#613))\n",
            "      :- Filter isnotnull(dttm#2205)\n",
            "      :  +- FileScan parquet spark_catalog.default.df1[dttm#2205,sin#2206,cos#2207] Batched: true, DataFilters: [isnotnull(dttm#2205)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "      +- Project [prev_dttm#604, prev_val#608, dttm#599 AS next_dttm#613, val#600 AS next_val#614]\n",
            "         +- Filter (isnotnull(prev_dttm#604) AND isnotnull(dttm#599))\n",
            "            +- Window [lag(dttm#599, -1, null) windowspecdefinition(dttm#599 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#604, lag(val#600, -1, null) windowspecdefinition(dttm#599 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#608], [dttm#599 ASC NULLS FIRST]\n",
            "               +- Sort [dttm#599 ASC NULLS FIRST], false, 0\n",
            "                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=737]\n",
            "                     +- FileScan parquet spark_catalog.default.df2[dttm#599,val#600] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manually emulate the Databricks optimization"
      ],
      "metadata": {
        "id": "rXXGCjM11EW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a minute for the bin size.\n",
        "# Smaller bins are faster, but less forgiving of missing data.\n",
        "spark.sql(\"\"\"\n",
        "  create or replace temp view df1b as\n",
        "  select *,\n",
        "    floor(unix_timestamp(dttm)/60) as bin\n",
        "  from df1\n",
        "\"\"\")\n",
        "spark.sql(\"\"\"\n",
        "  create or replace temp view df2b as\n",
        "  select *,\n",
        "    floor(unix_timestamp(prev_dttm)/60) as prev_bin,\n",
        "    floor(unix_timestamp(next_dttm)/60) as next_bin\n",
        "  from df2_ranges\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7UQTkAulepm",
        "outputId": "6ec4aa0b-a745-4539-a3d9-047056b8c27d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Much better plan.\n",
        "#\n",
        "# SortMergeJoin & hashpartitioning should be reasonable.\n",
        "\n",
        "manually_binned_join = spark.sql(\"\"\"\n",
        "WITH a as (\n",
        "   SELECT df1b.dttm as df1_dttm,\n",
        "         df1b.sin  as df1_sin,\n",
        "         df1b.cos  as df1_cos,\n",
        "         df2b.prev_dttm,\n",
        "         df2b.next_dttm,\n",
        "         df2b.prev_val,\n",
        "         df2b.next_val\n",
        "   FROM df1b\n",
        "   JOIN df2b ON (df1b.bin = df2b.next_bin)\n",
        "  ),\n",
        "  b as (\n",
        "  SELECT df1b.dttm as df1_dttm,\n",
        "         df1b.sin  as df1_sin,\n",
        "         df1b.cos  as df1_cos,\n",
        "         df2b.prev_dttm,\n",
        "         df2b.next_dttm,\n",
        "         df2b.prev_val,\n",
        "         df2b.next_val\n",
        "  FROM df1b\n",
        "  JOIN df2b ON (df1b.bin <> df2b.next_bin and df1b.bin = df2b.prev_bin)\n",
        "  ),\n",
        "  c as (\n",
        "    SELECT * FROM a\n",
        "    UNION ALL\n",
        "    SELECT * FROM b\n",
        "  )\n",
        "  SELECT * FROM c\n",
        "  where (df1_dttm >= prev_dttm and df1_dttm < next_dttm)\n",
        "\"\"\")\n",
        "manually_binned_join.explain()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF6AuHOXdiao",
        "outputId": "bd0a2882-8127-4584-cf87-064719916cf9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Union\n",
            "   :- Project [dttm#2476 AS df1_dttm#2449, sin#2477 AS df1_sin#2450, cos#2478 AS df1_cos#2451, prev_dttm#604 AS prev_dttm#2452, next_dttm#2425 AS next_dttm#2453, prev_val#608 AS prev_val#2454, next_val#2426 AS next_val#2455]\n",
            "   :  +- SortMergeJoin [bin#2416L], [next_bin#2428L], Inner, ((dttm#2476 >= prev_dttm#604) AND (dttm#2476 < next_dttm#2425))\n",
            "   :     :- Sort [bin#2416L ASC NULLS FIRST], false, 0\n",
            "   :     :  +- Exchange hashpartitioning(bin#2416L, 200), ENSURE_REQUIREMENTS, [plan_id=865]\n",
            "   :     :     +- Project [dttm#2476, sin#2477, cos#2478, FLOOR((cast(unix_timestamp(dttm#2476, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS bin#2416L]\n",
            "   :     :        +- Filter (isnotnull(dttm#2476) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#2476, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "   :     :           +- FileScan parquet spark_catalog.default.df1[dttm#2476,sin#2477,cos#2478] Batched: true, DataFilters: [isnotnull(dttm#2476), isnotnull(FLOOR((cast(unix_timestamp(dttm#2476, yyyy-MM-dd HH:mm:ss, Some(..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "   :     +- Sort [next_bin#2428L ASC NULLS FIRST], false, 0\n",
            "   :        +- Exchange hashpartitioning(next_bin#2428L, 200), ENSURE_REQUIREMENTS, [plan_id=866]\n",
            "   :           +- Project [prev_dttm#604, prev_val#608, dttm#2479 AS next_dttm#2425, val#2480 AS next_val#2426, FLOOR((cast(unix_timestamp(dttm#2479, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS next_bin#2428L]\n",
            "   :              +- Filter ((isnotnull(prev_dttm#604) AND isnotnull(dttm#2479)) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#2479, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "   :                 +- Window [lag(dttm#2479, -1, null) windowspecdefinition(dttm#2479 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#604, lag(val#2480, -1, null) windowspecdefinition(dttm#2479 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#608], [dttm#2479 ASC NULLS FIRST]\n",
            "   :                    +- Sort [dttm#2479 ASC NULLS FIRST], false, 0\n",
            "   :                       +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=858]\n",
            "   :                          +- FileScan parquet spark_catalog.default.df2[dttm#2479,val#2480] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "   +- Project [dttm#2481 AS df1_dttm#2489, sin#2482 AS df1_sin#2490, cos#2483 AS df1_cos#2491, prev_dttm#604, next_dttm#2442, prev_val#608, next_val#2443]\n",
            "      +- SortMergeJoin [bin#2433L], [prev_bin#2444L], Inner, (((dttm#2481 >= prev_dttm#604) AND (dttm#2481 < next_dttm#2442)) AND NOT (bin#2433L = next_bin#2445L))\n",
            "         :- Sort [bin#2433L ASC NULLS FIRST], false, 0\n",
            "         :  +- Exchange hashpartitioning(bin#2433L, 200), ENSURE_REQUIREMENTS, [plan_id=878]\n",
            "         :     +- Project [dttm#2481, sin#2482, cos#2483, FLOOR((cast(unix_timestamp(dttm#2481, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS bin#2433L]\n",
            "         :        +- Filter (isnotnull(dttm#2481) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#2481, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "         :           +- FileScan parquet spark_catalog.default.df1[dttm#2481,sin#2482,cos#2483] Batched: true, DataFilters: [isnotnull(dttm#2481), isnotnull(FLOOR((cast(unix_timestamp(dttm#2481, yyyy-MM-dd HH:mm:ss, Some(..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "         +- Sort [prev_bin#2444L ASC NULLS FIRST], false, 0\n",
            "            +- Exchange hashpartitioning(prev_bin#2444L, 200), ENSURE_REQUIREMENTS, [plan_id=879]\n",
            "               +- Project [prev_dttm#604, prev_val#608, dttm#2484 AS next_dttm#2442, val#2485 AS next_val#2443, FLOOR((cast(unix_timestamp(prev_dttm#604, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS prev_bin#2444L, FLOOR((cast(unix_timestamp(dttm#2484, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS next_bin#2445L]\n",
            "                  +- Filter ((((NOT (FLOOR((cast(unix_timestamp(prev_dttm#604, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) = FLOOR((cast(unix_timestamp(dttm#2484, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))) AND isnotnull(prev_dttm#604)) AND isnotnull(dttm#2484)) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#2484, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)))) AND isnotnull(FLOOR((cast(unix_timestamp(prev_dttm#604, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "                     +- Window [lag(dttm#2484, -1, null) windowspecdefinition(dttm#2484 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#604, lag(val#2485, -1, null) windowspecdefinition(dttm#2484 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#608], [dttm#2484 ASC NULLS FIRST]\n",
            "                        +- Sort [dttm#2484 ASC NULLS FIRST], false, 0\n",
            "                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=871]\n",
            "                              +- FileScan parquet spark_catalog.default.df2[dttm#2484,val#2485] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas\n",
        "t0 = time.time()\n",
        "manually_binned_join.createOrReplaceTempView(\"manually_binned_join\")\n",
        "result = spark.sql(\"select * from manually_binned_join order by df1_dttm\").limit(10).toPandas()\n",
        "t1 = time.time()\n",
        "print(f\"took {t1-t0:.2f} seconds\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "YuW177XkhDnx",
        "outputId": "74fbf5ec-6cd6-4002-d1c1-315e79513f83"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 12.32 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 df1_dttm   df1_sin   df1_cos           prev_dttm  \\\n",
              "0 2024-01-01 00:00:00.000  0.000000  1.000000 2024-01-01 00:00:00   \n",
              "1 2024-01-01 00:00:00.250  0.004167  0.999991 2024-01-01 00:00:00   \n",
              "2 2024-01-01 00:00:00.500  0.008333  0.999965 2024-01-01 00:00:00   \n",
              "3 2024-01-01 00:00:00.750  0.012500  0.999922 2024-01-01 00:00:00   \n",
              "4 2024-01-01 00:00:01.000  0.016666  0.999861 2024-01-01 00:00:01   \n",
              "5 2024-01-01 00:00:01.250  0.020832  0.999783 2024-01-01 00:00:01   \n",
              "6 2024-01-01 00:00:01.500  0.024997  0.999688 2024-01-01 00:00:01   \n",
              "7 2024-01-01 00:00:01.750  0.029163  0.999575 2024-01-01 00:00:01   \n",
              "8 2024-01-01 00:00:02.000  0.033327  0.999444 2024-01-01 00:00:02   \n",
              "9 2024-01-01 00:00:02.250  0.037491  0.999297 2024-01-01 00:00:02   \n",
              "\n",
              "            next_dttm  prev_val  next_val  \n",
              "0 2024-01-01 00:00:01  1.000000  0.999861  \n",
              "1 2024-01-01 00:00:01  1.000000  0.999861  \n",
              "2 2024-01-01 00:00:01  1.000000  0.999861  \n",
              "3 2024-01-01 00:00:01  1.000000  0.999861  \n",
              "4 2024-01-01 00:00:02  0.999861  0.999444  \n",
              "5 2024-01-01 00:00:02  0.999861  0.999444  \n",
              "6 2024-01-01 00:00:02  0.999861  0.999444  \n",
              "7 2024-01-01 00:00:02  0.999861  0.999444  \n",
              "8 2024-01-01 00:00:03  0.999444  0.998750  \n",
              "9 2024-01-01 00:00:03  0.999444  0.998750  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-895ffbbf-4563-4cc3-8dfb-2292607960f6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>df1_dttm</th>\n",
              "      <th>df1_sin</th>\n",
              "      <th>df1_cos</th>\n",
              "      <th>prev_dttm</th>\n",
              "      <th>next_dttm</th>\n",
              "      <th>prev_val</th>\n",
              "      <th>next_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-01-01 00:00:00.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:00.250</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.999991</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:00.500</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.999965</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-01-01 00:00:00.750</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-01-01 00:00:01.000</td>\n",
              "      <td>0.016666</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>0.999444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-01-01 00:00:01.250</td>\n",
              "      <td>0.020832</td>\n",
              "      <td>0.999783</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>0.999444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2024-01-01 00:00:01.500</td>\n",
              "      <td>0.024997</td>\n",
              "      <td>0.999688</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>0.999444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2024-01-01 00:00:01.750</td>\n",
              "      <td>0.029163</td>\n",
              "      <td>0.999575</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>0.999444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2024-01-01 00:00:02.000</td>\n",
              "      <td>0.033327</td>\n",
              "      <td>0.999444</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>2024-01-01 00:00:03</td>\n",
              "      <td>0.999444</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2024-01-01 00:00:02.250</td>\n",
              "      <td>0.037491</td>\n",
              "      <td>0.999297</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>2024-01-01 00:00:03</td>\n",
              "      <td>0.999444</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-895ffbbf-4563-4cc3-8dfb-2292607960f6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-895ffbbf-4563-4cc3-8dfb-2292607960f6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-895ffbbf-4563-4cc3-8dfb-2292607960f6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e367c15e-2d0c-40c3-9081-75960d9d962f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e367c15e-2d0c-40c3-9081-75960d9d962f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e367c15e-2d0c-40c3-9081-75960d9d962f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_529cfbcb-1342-43fb-95f6-c446175680e3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_529cfbcb-1342-43fb-95f6-c446175680e3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"df1_dttm\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-01-01 00:00:00\",\n        \"max\": \"2024-01-01 00:00:02.250000\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2024-01-01 00:00:02\",\n          \"2024-01-01 00:00:00.250000\",\n          \"2024-01-01 00:00:01.250000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"df1_sin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012612457762588535,\n        \"min\": 0.0,\n        \"max\": 0.037491211555460265,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.03332716083675362,\n          0.004166654610349972,\n          0.020831826325142813\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"df1_cos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002456754599775781,\n        \"min\": 0.9992969573935987,\n        \"max\": 1.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9994444958828685,\n          0.9999913194570031,\n          0.9997829939601689\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prev_dttm\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-01-01 00:00:00\",\n        \"max\": \"2024-01-01 00:00:02\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2024-01-01 00:00:00\",\n          \"2024-01-01 00:00:01\",\n          \"2024-01-01 00:00:02\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"next_dttm\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-01-01 00:00:01\",\n        \"max\": \"2024-01-01 00:00:03\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2024-01-01 00:00:01\",\n          \"2024-01-01 00:00:02\",\n          \"2024-01-01 00:00:03\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prev_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00021514526611270436,\n        \"min\": 0.9994444958828685,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.9998611143261019,\n          0.9994444958828685\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"next_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00042823682165277427,\n        \"min\": 0.9987502603949663,\n        \"max\": 0.9998611143261019,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9998611143261019,\n          0.9994444958828685,\n          0.9987502603949663\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSGsULlBi6Pf"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}