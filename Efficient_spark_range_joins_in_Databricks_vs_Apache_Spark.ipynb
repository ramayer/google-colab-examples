{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO0hQ/pGBsVGO1WIzEWj/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramayer/google-colab-examples/blob/main/Efficient_spark_range_joins_in_Databricks_vs_Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Efficient spark range joins in Databricks vs Apache Spark\n",
        "\n",
        "## Databricks spark has efficient range joins through hints.\n",
        "\n",
        "https://docs.databricks.com/en/optimizations/range-join.html\n",
        "\n",
        "\n",
        "## Apache Spark seems not to, yet.\n",
        "\n",
        "* https://github.com/apache/spark/pull/7379\n",
        "* https://issues.apache.org/jira/browse/SPARK-8682\n",
        "\n",
        "Zach Moshe describes a workaround.\n",
        "\n",
        "* http://zachmoshe.com/2016/09/26/efficient-range-joins-with-spark.html\n",
        "\n",
        "This notebook implements something simlar to each of the above.\n"
      ],
      "metadata": {
        "id": "BZs6cHBij5rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmn5B1KWWNBb",
        "outputId": "142d9599-6a8f-4403-b5b7-982ebc11e777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import pyspark, findspark, delta\n",
        "except:\n",
        "   %pip install -q --upgrade pyspark==3.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "\n",
        "MAX_MEMORY=\"8g\"\n",
        "maven_coords = [\n",
        "    'io.delta:delta-spark_2.12:3.2.0',\n",
        "]\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
        "    .config(\"spark.jars.packages\", \",\".join(maven_coords))\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY)\n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY)\n",
        "    .enableHiveSupport()\n",
        "    .getOrCreate()\n",
        "    )\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "0jcFcg-IWn0k",
        "outputId": "12516944-1df6-40d7-9285-f99a8efb4180"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d0a082a2950>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e16041c241e8:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.range(1000 * 1000).createOrReplaceTempView(\"a_million_rows\")"
      ],
      "metadata": {
        "id": "ClFoBZrRWkOs"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.sql(\"\"\"\n",
        "  select\n",
        "    cast('2024-01-01' as timestamp) + interval '1 second' * id /4 as dttm,\n",
        "    sin(id/60 / 4) as sin,\n",
        "    cos(id/60 / 4) as cos\n",
        "  from a_million_rows\n",
        "  \"\"\")\n",
        "df1.write.format(\"delta\").mode('overwrite').saveAsTable(\"df1\")\n",
        "df1.sort('dttm').limit(3).pandas_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "cFd42NeUWkSW",
        "outputId": "567b4202-165b-4dcd-839b-1b77f2882e88"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     dttm       sin       cos\n",
              "0 2024-01-01 00:00:00.000  0.000000  1.000000\n",
              "1 2024-01-01 00:00:00.250  0.004167  0.999991\n",
              "2 2024-01-01 00:00:00.500  0.008333  0.999965"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dttm</th>\n",
              "      <th>sin</th>\n",
              "      <th>cos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-01-01 00:00:00.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:00.250</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.999991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:00.500</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.999965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.sql(\"\"\"\n",
        "  select\n",
        "    cast('2024-01-01' as timestamp) + interval '1 second' * id as dttm,\n",
        "    cos(id*1.5/60) as val\n",
        "  from a_million_rows\n",
        "  \"\"\")\n",
        "df2.write.format(\"delta\").mode('overwrite').saveAsTable(\"df2\")\n",
        "df2.sort('dttm').limit(3).pandas_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "OHsVK6SgZmqg",
        "outputId": "ae2e411e-c7b4-47c3-87c4-940de51f9502"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 dttm       val\n",
              "0 2024-01-01 00:00:00  1.000000\n",
              "1 2024-01-01 00:00:01  0.999688\n",
              "2 2024-01-01 00:00:02  0.998750"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dttm</th>\n",
              "      <th>val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>0.999688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.table(\"df1\")\n",
        "df2 = spark.table(\"df2\")"
      ],
      "metadata": {
        "id": "B6htjGUViVYw"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_timestamp, lag\n",
        "from pyspark.sql.window import Window\n",
        "df2_ranges = (spark.sql(\"select * from df2\")\n",
        "                .withColumn(\"prev_dttm\", lag(\"dttm\").over(Window.orderBy(\"dttm\")))\n",
        "                .withColumn(\"prev_val\", lag(\"val\").over(Window.orderBy(\"dttm\")))\n",
        "                .selectExpr(\"prev_dttm\",\"prev_val\", \"dttm as next_dttm\", \"val as next_val\")\n",
        "                )\n",
        "df2_ranges.createOrReplaceTempView(\"df2_ranges\")\n",
        "df2_ranges.sort('prev_dttm').limit(3).pandas_api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "x67yts5Ua5pd",
        "outputId": "0a4732f1-68ed-47a0-de47-cb99b406b06c"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            prev_dttm  prev_val           next_dttm  next_val\n",
              "0                 NaT       NaN 2024-01-01 00:00:00  1.000000\n",
              "1 2024-01-01 00:00:00  1.000000 2024-01-01 00:00:01  0.999688\n",
              "2 2024-01-01 00:00:01  0.999688 2024-01-01 00:00:02  0.998750"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prev_dttm</th>\n",
              "      <th>prev_val</th>\n",
              "      <th>next_dttm</th>\n",
              "      <th>next_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>0.999688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>0.999688</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfkUYeasf9v9"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't run this on F/OSS Apache Spark, it takes forever\n",
        "# CartesianProduct and/or BroadcastNestedLoopJoin\n",
        "# (depending on spark configs) are both painful ways to do joins.\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "  SELECT df1.dttm as df1_dttm,\n",
        "         df1.sin  as df1_sin,\n",
        "         df1.cos  as df1_cos,\n",
        "         df2_ranges.prev_dttm,\n",
        "         df2_ranges.next_dttm,\n",
        "         df2_ranges.prev_val,\n",
        "         df2_ranges.next_val\n",
        "  FROM df1\n",
        "  JOIN df2_ranges ON (df1.dttm >= df2_ranges.prev_dttm\n",
        "                  AND df1.dttm <= df2_ranges.next_dttm)\n",
        "\"\"\").explain()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5SfzQSacEBY",
        "outputId": "7a5e5efe-21de-4b40-9820-9e8a5ad81e23"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [dttm#33021 AS df1_dttm#33018, sin#33022 AS df1_sin#33019, cos#33023 AS df1_cos#33020, prev_dttm#32119, next_dttm#32128, prev_val#32123, next_val#32129]\n",
            "   +- CartesianProduct ((dttm#33021 >= prev_dttm#32119) AND (dttm#33021 <= next_dttm#32128))\n",
            "      :- Filter isnotnull(dttm#33021)\n",
            "      :  +- FileScan parquet spark_catalog.default.df1[dttm#33021,sin#33022,cos#33023] Batched: true, DataFilters: [isnotnull(dttm#33021)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "      +- Project [prev_dttm#32119, prev_val#32123, dttm#32114 AS next_dttm#32128, val#32115 AS next_val#32129]\n",
            "         +- Filter (isnotnull(prev_dttm#32119) AND isnotnull(dttm#32114))\n",
            "            +- Window [lag(dttm#32114, -1, null) windowspecdefinition(dttm#32114 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#32119, lag(val#32115, -1, null) windowspecdefinition(dttm#32114 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#32123], [dttm#32114 ASC NULLS FIRST]\n",
            "               +- Sort [dttm#32114 ASC NULLS FIRST], false, 0\n",
            "                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=19273]\n",
            "                     +- FileScan parquet spark_catalog.default.df2[dttm#32114,val#32115] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python API does no better, even with a hint\n",
        "df1.hint(\"range_join\",6).join(df2_ranges,\n",
        "                               on=[\n",
        "                                   df1.dttm >= df2_ranges.prev_dttm,\n",
        "                                   df1.dttm <= df2_ranges.next_dttm\n",
        "                               ]).explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv1deS6umzPd",
        "outputId": "5ef41a00-ee4b-4f0a-abbc-a0e4c9eb5eb0"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- CartesianProduct ((dttm#32104 >= prev_dttm#32119) AND (dttm#32104 <= next_dttm#32128))\n",
            "   :- Filter isnotnull(dttm#32104)\n",
            "   :  +- FileScan parquet spark_catalog.default.df1[dttm#32104,sin#32105,cos#32106] Batched: true, DataFilters: [isnotnull(dttm#32104)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "   +- Project [prev_dttm#32119, prev_val#32123, dttm#32114 AS next_dttm#32128, val#32115 AS next_val#32129]\n",
            "      +- Filter (isnotnull(prev_dttm#32119) AND isnotnull(dttm#32114))\n",
            "         +- Window [lag(dttm#32114, -1, null) windowspecdefinition(dttm#32114 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#32119, lag(val#32115, -1, null) windowspecdefinition(dttm#32114 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#32123], [dttm#32114 ASC NULLS FIRST]\n",
            "            +- Sort [dttm#32114 ASC NULLS FIRST], false, 0\n",
            "               +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=19349]\n",
            "                  +- FileScan parquet spark_catalog.default.df2[dttm#32114,val#32115] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This should be fast on Databricks\n",
        "# https://docs.databricks.com/en/optimizations/range-join.html\n",
        "# but is annoyingly not on Apache Spark\n",
        "spark.sql(\"\"\"\n",
        "  SELECT  /*+ RANGE_JOIN(dttm, 10) */\n",
        "         df1.dttm as df1_dttm,\n",
        "         df1.sin  as df1_sin,\n",
        "         df1.cos  as df1_cos,\n",
        "         df2_ranges.prev_dttm,\n",
        "         df2_ranges.next_dttm,\n",
        "         df2_ranges.prev_val,\n",
        "         df2_ranges.next_val\n",
        "  FROM df1\n",
        "  JOIN df2_ranges ON (df1.dttm >= df2_ranges.prev_dttm\n",
        "                  AND df1.dttm <= df2_ranges.next_dttm)\n",
        "\"\"\").explain()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6ERU1jGgBOU",
        "outputId": "fccaff17-3071-4728-e466-cb1a5bf2142f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [dttm#33704 AS df1_dttm#33701, sin#33705 AS df1_sin#33702, cos#33706 AS df1_cos#33703, prev_dttm#32119, next_dttm#32128, prev_val#32123, next_val#32129]\n",
            "   +- CartesianProduct ((dttm#33704 >= prev_dttm#32119) AND (dttm#33704 <= next_dttm#32128))\n",
            "      :- Filter isnotnull(dttm#33704)\n",
            "      :  +- FileScan parquet spark_catalog.default.df1[dttm#33704,sin#33705,cos#33706] Batched: true, DataFilters: [isnotnull(dttm#33704)], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "      +- Project [prev_dttm#32119, prev_val#32123, dttm#32114 AS next_dttm#32128, val#32115 AS next_val#32129]\n",
            "         +- Filter (isnotnull(prev_dttm#32119) AND isnotnull(dttm#32114))\n",
            "            +- Window [lag(dttm#32114, -1, null) windowspecdefinition(dttm#32114 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#32119, lag(val#32115, -1, null) windowspecdefinition(dttm#32114 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#32123], [dttm#32114 ASC NULLS FIRST]\n",
            "               +- Sort [dttm#32114 ASC NULLS FIRST], false, 0\n",
            "                  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=19429]\n",
            "                     +- FileScan parquet spark_catalog.default.df2[dttm#32114,val#32115] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manually emulate the Databricks optimization"
      ],
      "metadata": {
        "id": "rXXGCjM11EW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a minute for the bin size.\n",
        "# Smaller bins are faster, but less forgiving of missing data.\n",
        "spark.sql(\"\"\"\n",
        "  create or replace temp view df1b as\n",
        "  select *,\n",
        "    floor(unix_timestamp(dttm)/60) as bin\n",
        "  from df1\n",
        "\"\"\")\n",
        "spark.sql(\"\"\"\n",
        "  create or replace temp view df2b as\n",
        "  select *,\n",
        "    floor(unix_timestamp(prev_dttm)/60) as prev_bin,\n",
        "    floor(unix_timestamp(next_dttm)/60) as next_bin\n",
        "  from df2_ranges\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7UQTkAulepm",
        "outputId": "2d1bfb30-ad56-47bd-b12b-97ac2b3d2484"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Much better plan.\n",
        "#\n",
        "# SortMergeJoin & hashpartitioning should be reasonable.\n",
        "\n",
        "manually_binned_join = spark.sql(\"\"\"\n",
        "WITH a as (\n",
        "   SELECT df1b.dttm as df1_dttm,\n",
        "         df1b.sin  as df1_sin,\n",
        "         df1b.cos  as df1_cos,\n",
        "         df2b.prev_dttm,\n",
        "         df2b.next_dttm,\n",
        "         df2b.prev_val,\n",
        "         df2b.next_val\n",
        "   FROM df1b\n",
        "   JOIN df2b ON (df1b.bin = df2b.next_bin)\n",
        "  ),\n",
        "  b as (\n",
        "  SELECT df1b.dttm as df1_dttm,\n",
        "         df1b.sin  as df1_sin,\n",
        "         df1b.cos  as df1_cos,\n",
        "         df2b.prev_dttm,\n",
        "         df2b.next_dttm,\n",
        "         df2b.prev_val,\n",
        "         df2b.next_val\n",
        "  FROM df1b\n",
        "  JOIN df2b ON (df1b.bin <> df2b.next_bin and df1b.bin = df2b.prev_bin)\n",
        "  ),\n",
        "  c as (\n",
        "    SELECT * FROM a\n",
        "    UNION ALL\n",
        "    SELECT * FROM b\n",
        "  )\n",
        "  SELECT * FROM c\n",
        "  where (df1_dttm >= prev_dttm and df1_dttm < next_dttm)\n",
        "\"\"\")\n",
        "manually_binned_join.explain()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF6AuHOXdiao",
        "outputId": "9fe2772f-5268-4e26-ee2b-622180f2ad7d"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Union\n",
            "   :- Project [dttm#33992 AS df1_dttm#33965, sin#33993 AS df1_sin#33966, cos#33994 AS df1_cos#33967, prev_dttm#32119 AS prev_dttm#33968, next_dttm#33941 AS next_dttm#33969, prev_val#32123 AS prev_val#33970, next_val#33942 AS next_val#33971]\n",
            "   :  +- SortMergeJoin [bin#33932L], [next_bin#33944L], Inner, ((dttm#33992 >= prev_dttm#32119) AND (dttm#33992 < next_dttm#33941))\n",
            "   :     :- Sort [bin#33932L ASC NULLS FIRST], false, 0\n",
            "   :     :  +- Exchange hashpartitioning(bin#33932L, 200), ENSURE_REQUIREMENTS, [plan_id=19557]\n",
            "   :     :     +- Project [dttm#33992, sin#33993, cos#33994, FLOOR((cast(unix_timestamp(dttm#33992, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS bin#33932L]\n",
            "   :     :        +- Filter (isnotnull(dttm#33992) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#33992, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "   :     :           +- FileScan parquet spark_catalog.default.df1[dttm#33992,sin#33993,cos#33994] Batched: true, DataFilters: [isnotnull(dttm#33992), isnotnull(FLOOR((cast(unix_timestamp(dttm#33992, yyyy-MM-dd HH:mm:ss, Som..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "   :     +- Sort [next_bin#33944L ASC NULLS FIRST], false, 0\n",
            "   :        +- Exchange hashpartitioning(next_bin#33944L, 200), ENSURE_REQUIREMENTS, [plan_id=19558]\n",
            "   :           +- Project [prev_dttm#32119, prev_val#32123, dttm#33995 AS next_dttm#33941, val#33996 AS next_val#33942, FLOOR((cast(unix_timestamp(dttm#33995, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS next_bin#33944L]\n",
            "   :              +- Filter ((isnotnull(prev_dttm#32119) AND isnotnull(dttm#33995)) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#33995, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "   :                 +- Window [lag(dttm#33995, -1, null) windowspecdefinition(dttm#33995 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#32119, lag(val#33996, -1, null) windowspecdefinition(dttm#33995 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#32123], [dttm#33995 ASC NULLS FIRST]\n",
            "   :                    +- Sort [dttm#33995 ASC NULLS FIRST], false, 0\n",
            "   :                       +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=19550]\n",
            "   :                          +- FileScan parquet spark_catalog.default.df2[dttm#33995,val#33996] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "   +- Project [dttm#33997 AS df1_dttm#34005, sin#33998 AS df1_sin#34006, cos#33999 AS df1_cos#34007, prev_dttm#32119, next_dttm#33958, prev_val#32123, next_val#33959]\n",
            "      +- SortMergeJoin [bin#33949L], [prev_bin#33960L], Inner, (((dttm#33997 >= prev_dttm#32119) AND (dttm#33997 < next_dttm#33958)) AND NOT (bin#33949L = next_bin#33961L))\n",
            "         :- Sort [bin#33949L ASC NULLS FIRST], false, 0\n",
            "         :  +- Exchange hashpartitioning(bin#33949L, 200), ENSURE_REQUIREMENTS, [plan_id=19570]\n",
            "         :     +- Project [dttm#33997, sin#33998, cos#33999, FLOOR((cast(unix_timestamp(dttm#33997, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS bin#33949L]\n",
            "         :        +- Filter (isnotnull(dttm#33997) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#33997, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "         :           +- FileScan parquet spark_catalog.default.df1[dttm#33997,sin#33998,cos#33999] Batched: true, DataFilters: [isnotnull(dttm#33997), isnotnull(FLOOR((cast(unix_timestamp(dttm#33997, yyyy-MM-dd HH:mm:ss, Som..., Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df1], PartitionFilters: [], PushedFilters: [IsNotNull(dttm)], ReadSchema: struct<dttm:timestamp,sin:double,cos:double>\n",
            "         +- Sort [prev_bin#33960L ASC NULLS FIRST], false, 0\n",
            "            +- Exchange hashpartitioning(prev_bin#33960L, 200), ENSURE_REQUIREMENTS, [plan_id=19571]\n",
            "               +- Project [prev_dttm#32119, prev_val#32123, dttm#34000 AS next_dttm#33958, val#34001 AS next_val#33959, FLOOR((cast(unix_timestamp(prev_dttm#32119, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS prev_bin#33960L, FLOOR((cast(unix_timestamp(dttm#34000, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) AS next_bin#33961L]\n",
            "                  +- Filter ((((NOT (FLOOR((cast(unix_timestamp(prev_dttm#32119, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)) = FLOOR((cast(unix_timestamp(dttm#34000, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))) AND isnotnull(prev_dttm#32119)) AND isnotnull(dttm#34000)) AND isnotnull(FLOOR((cast(unix_timestamp(dttm#34000, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0)))) AND isnotnull(FLOOR((cast(unix_timestamp(prev_dttm#32119, yyyy-MM-dd HH:mm:ss, Some(Etc/UTC), false) as double) / 60.0))))\n",
            "                     +- Window [lag(dttm#34000, -1, null) windowspecdefinition(dttm#34000 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_dttm#32119, lag(val#34001, -1, null) windowspecdefinition(dttm#34000 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS prev_val#32123], [dttm#34000 ASC NULLS FIRST]\n",
            "                        +- Sort [dttm#34000 ASC NULLS FIRST], false, 0\n",
            "                           +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=19563]\n",
            "                              +- FileScan parquet spark_catalog.default.df2[dttm#34000,val#34001] Batched: true, DataFilters: [], Format: Parquet, Location: PreparedDeltaFileIndex(1 paths)[file:/content/spark-warehouse/df2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<dttm:timestamp,val:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas\n",
        "t0 = time.time()\n",
        "manually_binned_join.createOrReplaceTempView(\"manually_binned_join\")\n",
        "result = spark.sql(\"select * from manually_binned_join order by df1_dttm\").limit(10).toPandas()\n",
        "t1 = time.time()\n",
        "print(f\"took {t1-t0:.2f} seconds\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "YuW177XkhDnx",
        "outputId": "8786461d-c6ae-4ebf-b731-4b4f5c9a56bf"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "took 6.50 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 df1_dttm   df1_sin   df1_cos           prev_dttm  \\\n",
              "0 2024-01-01 00:00:00.000  0.000000  1.000000 2024-01-01 00:00:00   \n",
              "1 2024-01-01 00:00:00.250  0.004167  0.999991 2024-01-01 00:00:00   \n",
              "2 2024-01-01 00:00:00.500  0.008333  0.999965 2024-01-01 00:00:00   \n",
              "3 2024-01-01 00:00:00.750  0.012500  0.999922 2024-01-01 00:00:00   \n",
              "4 2024-01-01 00:00:01.000  0.016666  0.999861 2024-01-01 00:00:01   \n",
              "5 2024-01-01 00:00:01.250  0.020832  0.999783 2024-01-01 00:00:01   \n",
              "6 2024-01-01 00:00:01.500  0.024997  0.999688 2024-01-01 00:00:01   \n",
              "7 2024-01-01 00:00:01.750  0.029163  0.999575 2024-01-01 00:00:01   \n",
              "8 2024-01-01 00:00:02.000  0.033327  0.999444 2024-01-01 00:00:02   \n",
              "9 2024-01-01 00:00:02.250  0.037491  0.999297 2024-01-01 00:00:02   \n",
              "\n",
              "            next_dttm  prev_val  next_val  \n",
              "0 2024-01-01 00:00:01  1.000000  0.999688  \n",
              "1 2024-01-01 00:00:01  1.000000  0.999688  \n",
              "2 2024-01-01 00:00:01  1.000000  0.999688  \n",
              "3 2024-01-01 00:00:01  1.000000  0.999688  \n",
              "4 2024-01-01 00:00:02  0.999688  0.998750  \n",
              "5 2024-01-01 00:00:02  0.999688  0.998750  \n",
              "6 2024-01-01 00:00:02  0.999688  0.998750  \n",
              "7 2024-01-01 00:00:02  0.999688  0.998750  \n",
              "8 2024-01-01 00:00:03  0.998750  0.997189  \n",
              "9 2024-01-01 00:00:03  0.998750  0.997189  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be5ce0e2-8867-4324-9ffd-1eb9ead0a887\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>df1_dttm</th>\n",
              "      <th>df1_sin</th>\n",
              "      <th>df1_cos</th>\n",
              "      <th>prev_dttm</th>\n",
              "      <th>next_dttm</th>\n",
              "      <th>prev_val</th>\n",
              "      <th>next_val</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-01-01 00:00:00.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-01-01 00:00:00.250</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.999991</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-01-01 00:00:00.500</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.999965</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-01-01 00:00:00.750</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>2024-01-01 00:00:00</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.999688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-01-01 00:00:01.000</td>\n",
              "      <td>0.016666</td>\n",
              "      <td>0.999861</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999688</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-01-01 00:00:01.250</td>\n",
              "      <td>0.020832</td>\n",
              "      <td>0.999783</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999688</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2024-01-01 00:00:01.500</td>\n",
              "      <td>0.024997</td>\n",
              "      <td>0.999688</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999688</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2024-01-01 00:00:01.750</td>\n",
              "      <td>0.029163</td>\n",
              "      <td>0.999575</td>\n",
              "      <td>2024-01-01 00:00:01</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>0.999688</td>\n",
              "      <td>0.998750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2024-01-01 00:00:02.000</td>\n",
              "      <td>0.033327</td>\n",
              "      <td>0.999444</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>2024-01-01 00:00:03</td>\n",
              "      <td>0.998750</td>\n",
              "      <td>0.997189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2024-01-01 00:00:02.250</td>\n",
              "      <td>0.037491</td>\n",
              "      <td>0.999297</td>\n",
              "      <td>2024-01-01 00:00:02</td>\n",
              "      <td>2024-01-01 00:00:03</td>\n",
              "      <td>0.998750</td>\n",
              "      <td>0.997189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be5ce0e2-8867-4324-9ffd-1eb9ead0a887')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be5ce0e2-8867-4324-9ffd-1eb9ead0a887 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be5ce0e2-8867-4324-9ffd-1eb9ead0a887');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-31d8d2b3-e21e-447f-b3f5-75c011f6ef43\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31d8d2b3-e21e-447f-b3f5-75c011f6ef43')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-31d8d2b3-e21e-447f-b3f5-75c011f6ef43 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_24368c98-6061-478e-8d31-66d575396f6b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_24368c98-6061-478e-8d31-66d575396f6b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result",
              "summary": "{\n  \"name\": \"result\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"df1_dttm\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-01-01 00:00:00\",\n        \"max\": \"2024-01-01 00:00:02.250000\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2024-01-01 00:00:02\",\n          \"2024-01-01 00:00:00.250000\",\n          \"2024-01-01 00:00:01.250000\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"df1_sin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012612457762588535,\n        \"min\": 0.0,\n        \"max\": 0.037491211555460265,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.03332716083675362,\n          0.004166654610349972,\n          0.020831826325142813\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"df1_cos\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0002456754599775781,\n        \"min\": 0.9992969573935987,\n        \"max\": 1.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9994444958828685,\n          0.9999913194570031,\n          0.9997829939601689\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prev_dttm\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-01-01 00:00:00\",\n        \"max\": \"2024-01-01 00:00:02\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2024-01-01 00:00:00\",\n          \"2024-01-01 00:00:01\",\n          \"2024-01-01 00:00:02\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"next_dttm\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-01-01 00:00:01\",\n        \"max\": \"2024-01-01 00:00:03\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2024-01-01 00:00:01\",\n          \"2024-01-01 00:00:02\",\n          \"2024-01-01 00:00:03\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prev_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00048401926704223986,\n        \"min\": 0.9987502603949663,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0,\n          0.9996875162757026,\n          0.9987502603949663\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"next_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.000963257975672635,\n        \"min\": 0.9971888181122075,\n        \"max\": 0.9996875162757026,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9996875162757026,\n          0.9987502603949663,\n          0.9971888181122075\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSGsULlBi6Pf"
      },
      "execution_count": 117,
      "outputs": []
    }
  ]
}