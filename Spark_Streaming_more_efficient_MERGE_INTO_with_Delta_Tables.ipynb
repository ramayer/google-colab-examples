{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Delta Tables more efficient MERGE INTO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramayer/google-colab-examples/blob/main/Spark_Streaming_more_efficient_MERGE_INTO_with_Delta_Tables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY"
      },
      "source": [
        "\n",
        "# Spark Delta Table more efficient MERGE INTO\n",
        "\n",
        "\n",
        "### Based on\n",
        "\n",
        "* https://docs.databricks.com/_static/notebooks/merge-in-streaming.html\n",
        "* https://github.com/delta-io/delta/issues/490\n",
        "* https://kb.databricks.com/delta/delta-merge-into.html\n",
        "* https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-merge0-into\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaplmdWPO02r"
      },
      "source": [
        "#### install Java and Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-e1DAsGUaL"
      },
      "source": [
        "!apt-get -qq install -y openjdk-8-jdk-headless > /tmp/apt-get.out\n",
        "!(wget -q --show-progress -nc https://mirrors.ocf.berkeley.edu/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz)\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGBmI9oxWCEV"
      },
      "source": [
        "## Install pyspark and related python libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2hVj29_H4NC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22fb8a0a-12f4-4806-a1cc-4a24697f731b"
      },
      "source": [
        "try:\n",
        "  import pyspark, findspark, delta\n",
        "except:\n",
        "  %pip install -q --upgrade pyspark findspark delta\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 39 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 46.5 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for delta (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P"
      },
      "source": [
        "# Start a Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "97ed1f4f-a178-44fa-95b7-f99eb9309107"
      },
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "# Reasonable for tiny one-node Spark \"cluster\" in Google Colab notebooks\n",
        "MAX_MEMORY=\"8g\"\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\") \n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY) \n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY) \n",
        "    .enableHiveSupport() \n",
        "    .getOrCreate()        \n",
        "    )\n",
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0d5b137eff16:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fdb7b5bd9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w0vF5yhaM6U"
      },
      "source": [
        "## Create a spark streaming pipeline streaming data from Bronze -> Silver -> Gold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyLUSLvHJOoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d666e9-f7b6-49c2-aac5-6a4e897c1127"
      },
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS spark_streaming_bronze\")\n",
        "spark.sql(\"\"\"\n",
        "   create table if not exists spark_streaming_bronze (\n",
        "     id            bigint,\n",
        "     version_id    int,\n",
        "     partition_id  int, -- generated always as (id %10) \n",
        "     ts            timestamp,\n",
        "     data          string\n",
        "   ) USING DELTA;\n",
        "\"\"\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twGQWmp3aVmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6668fc3-4f21-4a5c-f6a1-24c02929d722"
      },
      "source": [
        "!ps"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:00 docker-init\n",
            "      7 ?        00:00:06 node\n",
            "     17 ?        00:00:00 tail\n",
            "     35 ?        00:00:00 colab-fileshim.\n",
            "     48 ?        00:00:07 jupyter-noteboo\n",
            "     49 ?        00:00:04 dap_multiplexer\n",
            "     59 ?        00:00:25 python3\n",
            "     79 ?        00:00:09 python3\n",
            "    251 ?        00:05:03 java\n",
            "   2194 ?        00:00:00 ps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE9ZPouLMyEZ"
      },
      "source": [
        "df = spark.read.table(\"spark_streaming_bronze\")\n",
        "df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"spark_streaming_silver_naive\")\n",
        "df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"spark_streaming_silver_theoretically_better\")\n",
        "df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"spark_streaming_silver_actually_better\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkmsfcCzyvVl",
        "outputId": "3bde6a10-f629-42f0-c94f-2fe61794db24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "  insert into spark_streaming_bronze (select id, 1, id%10, now(), 'data for '||id  from range(100000))\n",
        "\"\"\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUAsPpX-aYxl"
      },
      "source": [
        "## Set up the streaming operations from Bronze to Silver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWuO4dzV0c73"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6XZSSPp0c5O",
        "outputId": "76f2dff2-32b4-4594-b38a-569a82f82a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bronze_input_stream = (spark.readStream.format(\"delta\")\n",
        "    .option(\"maxFilesPerTrigger\",10)\n",
        "    .option(\"maxBytesPerTrigger\",1_000_000_000)\n",
        "    .option(\"ignoreChanges\",\"True\")\n",
        "    .table(\"spark_streaming_bronze\")\n",
        "    )\n",
        "bronze_input_stream.isStreaming"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o76oIwdS1XOw",
        "outputId": "18f0683a-71ae-4cb9-d6c6-e2ac134898c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "outstr = (bronze_input_stream.writeStream\n",
        "  .format(\"delta\")\n",
        "  .outputMode(\"append\")\n",
        "  .option(\"checkpointLocation\", \"/tmp/s1\")\n",
        "  .toTable(\"events\")\n",
        ")\n",
        "outstr"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x7fdb75669310>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-JuqARR1XLB",
        "outputId": "5492fc6a-20bd-43d8-de73-b2053177885a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spark.sql(\"select * from events\").show()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------------+--------------------+-----------+\n",
            "| id|version_id|partition_id|                  ts|       data|\n",
            "+---+----------+------------+--------------------+-----------+\n",
            "|  0|         1|           0|2021-11-26 10:32:...| data for 0|\n",
            "|  1|         1|           1|2021-11-26 10:32:...| data for 1|\n",
            "|  2|         1|           2|2021-11-26 10:32:...| data for 2|\n",
            "|  3|         1|           3|2021-11-26 10:32:...| data for 3|\n",
            "|  4|         1|           4|2021-11-26 10:32:...| data for 4|\n",
            "|  5|         1|           5|2021-11-26 10:32:...| data for 5|\n",
            "|  6|         1|           6|2021-11-26 10:32:...| data for 6|\n",
            "|  7|         1|           7|2021-11-26 10:32:...| data for 7|\n",
            "|  8|         1|           8|2021-11-26 10:32:...| data for 8|\n",
            "|  9|         1|           9|2021-11-26 10:32:...| data for 9|\n",
            "| 10|         1|           0|2021-11-26 10:32:...|data for 10|\n",
            "| 11|         1|           1|2021-11-26 10:32:...|data for 11|\n",
            "| 12|         1|           2|2021-11-26 10:32:...|data for 12|\n",
            "| 13|         1|           3|2021-11-26 10:32:...|data for 13|\n",
            "| 14|         1|           4|2021-11-26 10:32:...|data for 14|\n",
            "| 15|         1|           5|2021-11-26 10:32:...|data for 15|\n",
            "| 16|         1|           6|2021-11-26 10:32:...|data for 16|\n",
            "| 17|         1|           7|2021-11-26 10:32:...|data for 17|\n",
            "| 18|         1|           8|2021-11-26 10:32:...|data for 18|\n",
            "| 19|         1|           9|2021-11-26 10:32:...|data for 19|\n",
            "+---+----------+------------+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmScijXt1XDZ"
      },
      "source": [
        "o = o.option(\"checkpointLocation\", \"/tmp/s1\")\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS60azXO-mGN",
        "outputId": "87ce45bc-942d-4d17-801b-e8cb9119f7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def upsertToDelta(microBatchOutputDF, batchId): \n",
        "  '''\n",
        "    Naive approach from https://docs.databricks.com/_static/notebooks/merge-in-streaming.html\n",
        "    Can result in many unnecessary rows streamed downstream.\n",
        "  '''\n",
        "  microBatchOutputDF.createOrReplaceTempView(\"updates\")\n",
        "  microBatchOutputDF._jdf.sparkSession().sql(\"\"\"\n",
        "    MERGE INTO spark_streaming_silver_naive t\n",
        "    USING updates s\n",
        "    ON s.id = t.id\n",
        "    WHEN MATCHED THEN UPDATE SET *\n",
        "    WHEN NOT MATCHED THEN INSERT *\n",
        "  \"\"\")\n",
        "  return 1\n",
        "\n",
        "naive_output_stream = (bronze_input_stream.writeStream\n",
        "                       .format(\"delta\")\n",
        "                       #.trigger(processingTime='5 seconds')\n",
        "                       .trigger(once=True)\n",
        "                       .option(\"checkpointLocation\",\"/tmp/naive_checkpoint_2\")\n",
        "                       .foreachBatch(upsertToDelta)\n",
        "                       .outputMode(\"update\")\n",
        "                       .start()\n",
        ")\n",
        "naive_output_stream.status"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'isDataAvailable': False,\n",
              " 'isTriggerActive': False,\n",
              " 'message': 'Initializing sources'}"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knEn-ZqBAQRH",
        "outputId": "7b467cdf-eb3c-4094-8976-8b77e19d371a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "naive_output_stream.status"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'isDataAvailable': False,\n",
              " 'isTriggerActive': False,\n",
              " 'message': 'Terminated with exception: Error while obtaining a new communication channel'}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3xtTV0E7A0x",
        "outputId": "6c75ffbf-57f4-43b6-a4bd-a464d88ccdba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        ""
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-d7e509695fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'executor'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF40FXhBcffS"
      },
      "source": [
        "# Show table histories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0gmY7adcl_X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "7810a91a-a36d-47fa-f2a1-3b7dea46ef03"
      },
      "source": [
        "import IPython.display\n",
        "import html\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "def html_escape(c):\n",
        "  if isinstance(c,datetime.datetime):\n",
        "    return c.isoformat()\n",
        "  if isinstance(c,dict):\n",
        "    return \"<pre>\"+html.escape(json.dumps(c,indent=1))+\"</pre>\"\n",
        "  return html.escape(str(c))\n",
        "\n",
        "def df_to_html(df,rows=10,title = None):\n",
        "  html_rows = []\n",
        "  for idx,row in enumerate(df.take(rows)):\n",
        "    data = row.asDict(True)\n",
        "    if idx == 0:\n",
        "      cells = [html.escape(v) for v in data.keys()]\n",
        "      html_rows.append(\"<tr><th>\"+\"</th><th>\".join(cells)+\"</th></tr>\")\n",
        "    cells = [html_escape(v) for v in data.values()]\n",
        "    html_rows.append(\"<tr><td>\"+\"</td><td>\".join(cells)+\"</td></tr>\")\n",
        "  title_row = title and f\"<tr><th colspan={len(df.columns)}>{html.escape(title)}</th></tr>\"\n",
        "  h = \"<table>\" + (title_row or \"\") + (\"\\n\".join(html_rows)) + \"</table>\"\n",
        "  style = \"\"\"\n",
        "            <style>\n",
        "              tr {vertical-align:baseline;}\n",
        "              table {border-collapse: collapse;\tborder-spacing: 0;}\n",
        "              th, td {border: 1px solid black; padding:5px}\n",
        "            </style>\n",
        "          \"\"\"\n",
        "  return style + h\n",
        "\n",
        "\n",
        "def table_history(tbl):\n",
        "  interesting_fields = \"timestamp, operation, operationmetrics,operationParameters\".split(',')\n",
        "  return spark.sql(f\"\"\"describe history {tbl}\"\"\").selectExpr(interesting_fields)\n",
        "\n",
        "IPython.display.HTML(\n",
        "    df_to_html(table_history('spark_streaming_bronze')      ,title='source') +\n",
        "    df_to_html(table_history('spark_streaming_silver_naive'),title='silver_1') +\n",
        "    df_to_html(table_history('spark_streaming_bronze')      ,title='source')\n",
        ")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <style>\n",
              "              tr {vertical-align:baseline;}\n",
              "              table {border-collapse: collapse;\tborder-spacing: 0;}\n",
              "              th, td {border: 1px solid black; padding:5px}\n",
              "            </style>\n",
              "          <table><tr><th colspan=4>source</th></tr><tr><th>timestamp</th><th>operation</th><th>operationmetrics</th><th>operationParameters</th></tr>\n",
              "<tr><td>2021-11-26T10:32:27</td><td>WRITE</td><td><pre>{\n",
              " &quot;numOutputRows&quot;: &quot;100000&quot;,\n",
              " &quot;numOutputBytes&quot;: &quot;903476&quot;,\n",
              " &quot;numFiles&quot;: &quot;2&quot;\n",
              "}</pre></td><td><pre>{\n",
              " &quot;mode&quot;: &quot;Append&quot;,\n",
              " &quot;partitionBy&quot;: &quot;[]&quot;\n",
              "}</pre></td></tr>\n",
              "<tr><td>2021-11-26T10:14:31</td><td>CREATE TABLE</td><td><pre>{}</pre></td><td><pre>{\n",
              " &quot;description&quot;: null,\n",
              " &quot;partitionBy&quot;: &quot;[]&quot;,\n",
              " &quot;properties&quot;: &quot;{}&quot;,\n",
              " &quot;isManaged&quot;: &quot;true&quot;\n",
              "}</pre></td></tr></table>\n",
              "            <style>\n",
              "              tr {vertical-align:baseline;}\n",
              "              table {border-collapse: collapse;\tborder-spacing: 0;}\n",
              "              th, td {border: 1px solid black; padding:5px}\n",
              "            </style>\n",
              "          <table><tr><th colspan=4>silver_1</th></tr><tr><th>timestamp</th><th>operation</th><th>operationmetrics</th><th>operationParameters</th></tr>\n",
              "<tr><td>2021-11-26T10:15:04</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td><pre>{\n",
              " &quot;numOutputRows&quot;: &quot;0&quot;,\n",
              " &quot;numOutputBytes&quot;: &quot;632&quot;,\n",
              " &quot;numFiles&quot;: &quot;1&quot;\n",
              "}</pre></td><td><pre>{\n",
              " &quot;description&quot;: null,\n",
              " &quot;partitionBy&quot;: &quot;[]&quot;,\n",
              " &quot;properties&quot;: &quot;{}&quot;,\n",
              " &quot;isManaged&quot;: &quot;true&quot;\n",
              "}</pre></td></tr></table>\n",
              "            <style>\n",
              "              tr {vertical-align:baseline;}\n",
              "              table {border-collapse: collapse;\tborder-spacing: 0;}\n",
              "              th, td {border: 1px solid black; padding:5px}\n",
              "            </style>\n",
              "          <table><tr><th colspan=4>source</th></tr><tr><th>timestamp</th><th>operation</th><th>operationmetrics</th><th>operationParameters</th></tr>\n",
              "<tr><td>2021-11-26T10:32:27</td><td>WRITE</td><td><pre>{\n",
              " &quot;numOutputRows&quot;: &quot;100000&quot;,\n",
              " &quot;numOutputBytes&quot;: &quot;903476&quot;,\n",
              " &quot;numFiles&quot;: &quot;2&quot;\n",
              "}</pre></td><td><pre>{\n",
              " &quot;mode&quot;: &quot;Append&quot;,\n",
              " &quot;partitionBy&quot;: &quot;[]&quot;\n",
              "}</pre></td></tr>\n",
              "<tr><td>2021-11-26T10:14:31</td><td>CREATE TABLE</td><td><pre>{}</pre></td><td><pre>{\n",
              " &quot;description&quot;: null,\n",
              " &quot;partitionBy&quot;: &quot;[]&quot;,\n",
              " &quot;properties&quot;: &quot;{}&quot;,\n",
              " &quot;isManaged&quot;: &quot;true&quot;\n",
              "}</pre></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH6i9mjRvuej"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}