{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Delta Tables more efficient MERGE INTO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramayer/google-colab-examples/blob/main/Spark_Delta_Tables_more_efficient_MERGE_INTO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvD4HBMi0ohY"
      },
      "source": [
        "\n",
        "# Spark Delta Table more efficient MERGE INTO\n",
        "\n",
        "\n",
        "### Based on\n",
        "\n",
        "* https://docs.databricks.com/_static/notebooks/merge-in-streaming.html\n",
        "* https://github.com/delta-io/delta/issues/490\n",
        "* https://kb.databricks.com/delta/delta-merge-into.html\n",
        "* https://docs.microsoft.com/en-us/azure/databricks/kb/delta/delta-merge0-into\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaplmdWPO02r"
      },
      "source": [
        "#### install Java and Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-e1DAsGUaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbe114f-bad5-4977-ca45-0f9e7d766319"
      },
      "source": [
        "!apt-get -qq install -y openjdk-8-jdk-headless > /tmp/apt-get.out\n",
        "!(wget -q --show-progress -nc https://mirrors.ocf.berkeley.edu/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz)\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.1.2-bin-had 100%[===================>] 218.23M  55.4MB/s    in 4.2s    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGBmI9oxWCEV"
      },
      "source": [
        "## Install pyspark and related python libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2hVj29_H4NC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49f6182-4507-4ead-859e-d1cc348b7387"
      },
      "source": [
        "try:\n",
        "  import pyspark, findspark, delta\n",
        "except:\n",
        "  %pip install -q --upgrade pyspark findspark delta\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 41 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 32.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for delta (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwU28K5f1H3P"
      },
      "source": [
        "# Start a Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgReRGl0y23D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "b6ffdc39-1fdf-41a4-bb50-8b0021632ef1"
      },
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "# Reasonable for tiny one-node Spark \"cluster\" in Google Colab notebooks\n",
        "MAX_MEMORY=\"8g\"\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\") \n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY) \n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY) \n",
        "    .enableHiveSupport() \n",
        "    .getOrCreate()        \n",
        "    )\n",
        "spark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d3ebd555d8ff:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb68fb69b50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8e_3eO3uUIU",
        "outputId": "abf363cd-d79f-45bb-c3cb-5ada9401014e"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "   create table if not exists test_stream (id bigint, data  string) USING DELTA;\n",
        "\"\"\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzDefEMIuUEe"
      },
      "source": [
        "input_stream = (spark.readStream.format(\"delta\")\n",
        "    .option(\"maxFilesPerTrigger\",10)\n",
        "    .option(\"maxBytesPerTrigger\",1_000_000_000)\n",
        "    .option(\"ignoreChanges\",\"True\")\n",
        "    .table(\"test_stream\")\n",
        "    )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-8pK3V8uUAh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYIxCj9RSujM",
        "outputId": "c5c1fe40-5a54-4cba-ef7f-1375ed204594"
      },
      "source": [
        "spark.sql(\"DESCRIBE detail spark_streaming_bronze\").show()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
            "|format|                  id|                name|description|            location|           createdAt|       lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|\n",
            "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
            "| delta|e2909139-09e7-47a...|default.spark_str...|       null|file:/content/spa...|2021-11-26 03:07:...|2021-11-26 03:07:45|              []|       0|          0|        {}|               1|               2|\n",
            "+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w0vF5yhaM6U"
      },
      "source": [
        "## Create a spark streaming pipeline streaming data from Bronze -> Silver -> Gold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyLUSLvHJOoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ca369b-7a55-453e-9c4a-4a888e1f7b9e"
      },
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS spark_streaming_bronze\")\n",
        "spark.sql(\"\"\"\n",
        "   create table if not exists spark_streaming_bronze (\n",
        "     id            bigint,\n",
        "     version_id    int,\n",
        "     partition_id  int, -- generated always as (id %10) \n",
        "     ts            timestamp,\n",
        "     data          string\n",
        "   ) USING DELTA;\n",
        "\"\"\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twGQWmp3aVmh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "f36953d7-a211-4634-97e5-88a45cb2f469"
      },
      "source": [
        "bronze_input_stream = (spark.readStream.format(\"delta\")\n",
        "    .option(\"maxFilesPerTrigger\",10)\n",
        "    .option(\"maxBytesPerTrigger\",1_000_000_000)\n",
        "    .option(\"ignoreChanges\",\"True\")\n",
        "    .table(\"spark_streaming_bronze\")\n",
        "    )\n",
        "bronze_input_stream.isStreaming"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f30b1c9995ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maxBytesPerTrigger\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1_000_000_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignoreChanges\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark_streaming_bronze\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     )\n\u001b[1;32m      7\u001b[0m \u001b[0mbronze_input_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misStreaming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \"\"\"\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tableName can be only a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table default.spark_streaming_bronze does not support either micro-batch or continuous scan.;\nSubqueryAlias spark_catalog.default.spark_streaming_bronze\n+- StreamingRelationV2 default.spark_streaming_bronze, DeltaTableV2(org.apache.spark.sql.SparkSession@1f9bb0f9,file:/content/spark-warehouse/spark_streaming_bronze,Some(CatalogTable(\nDatabase: default\nTable: spark_streaming_bronze\nOwner: root\nCreated Time: Fri Nov 26 03:08:00 UTC 2021\nLast Access: UNKNOWN\nCreated By: Spark 3.1.2\nType: MANAGED\nProvider: delta\nLocation: file:/content/spark-warehouse/spark_streaming_bronze\nSerde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\nInputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat\nOutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\nPartition Provider: Catalog)),Some(default.spark_streaming_bronze),None,org.apache.spark.sql.util.CaseInsensitiveStringMap@1f), [maxFilesPerTrigger=10, ignoreChanges=True, maxBytesPerTrigger=1000000000], [id#2522L, version_id#2523, partition_id#2524, ts#2525, data#2526], org.apache.spark.sql.delta.catalog.DeltaCatalog@3fbcef5b, default.spark_streaming_bronze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE9ZPouLMyEZ"
      },
      "source": [
        "df = spark.read.table(\"spark_streaming_bronze\")\n",
        "df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"spark_streaming_silver_naive\")\n",
        "df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"spark_streaming_silver_theoretically_better\")\n",
        "df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"spark_streaming_silver_actually_better\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUAsPpX-aYxl"
      },
      "source": [
        "## Set up the streaming operations from Bronze to Silver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWgLuPtCsl0U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS60azXO-mGN"
      },
      "source": [
        "def upsertToDelta(microBatchOutputDF, batchId): \n",
        "  '''\n",
        "    Naive approach from https://docs.databricks.com/_static/notebooks/merge-in-streaming.html\n",
        "    Can result in many unnecessary rows streamed downstream.\n",
        "  '''\n",
        "  microBatchOutputDF.createOrReplaceTempView(\"updates\")\n",
        "  microBatchOutputDF._jdf.sparkSession().sql(\"\"\"\n",
        "    MERGE INTO spark_streaming_silver_naive t\n",
        "    USING updates s\n",
        "    ON s.key = t.key\n",
        "    WHEN MATCHED THEN UPDATE SET *\n",
        "    WHEN NOT MATCHED THEN INSERT *\n",
        "  \"\"\")\n",
        "  return 1\n",
        "\n",
        "naive_output_stream = (bronze_input_stream.writeStream\n",
        "                       .format(\"delta\")\n",
        "                       .trigger(processingTime='5 seconds')\n",
        "                       .option(\"checkpointLocation\",\"/tmp/naive_checkpoint_1\")\n",
        "                       .foreachBatch(upsertToDelta)\n",
        "                       .outputMode(\"update\")\n",
        "                       .start()\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knEn-ZqBAQRH"
      },
      "source": [
        "s = spark.streams.active[0]\n",
        "s.recentProgress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF40FXhBcffS"
      },
      "source": [
        "# Show table histories\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-EutxuYKZ1J"
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0gmY7adcl_X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "a7328e37-227f-4add-eed8-ef9b4e24382a"
      },
      "source": [
        "import IPython.display\n",
        "import html\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "def html_escape(c):\n",
        "  if isinstance(c,datetime.datetime):\n",
        "    return c.isoformat()\n",
        "  if isinstance(c,dict):\n",
        "    return \"<pre>\"+html.escape(json.dumps(c,indent=1))+\"</pre>\"\n",
        "  return html.escape(str(c))\n",
        "\n",
        "def df_to_html(df,rows=10,title = None):\n",
        "  html_rows = []\n",
        "  for idx,row in enumerate(hist_df.take(rows)):\n",
        "    data = row.asDict(True)\n",
        "    if idx == 0:\n",
        "      cells = [html.escape(v) for v in data.keys()]\n",
        "      html_rows.append(\"<tr><th>\"+\"</th><th>\".join(cells)+\"</th></tr>\")\n",
        "    cells = [html_escape(v) for v in data.values()]\n",
        "    html_rows.append(\"<tr><td>\"+\"</td><td>\".join(cells)+\"</td></tr>\")\n",
        "  h = \"<table>\" + (\"\\n\".join(html_rows)) + \"</table>\"\n",
        "  style = \"\"\"<style>td {border: 1px solid black}</style>\"\"\"\n",
        "  return \"<h2>\"+(title or '')+\"</h2>\"+style + h\n",
        "\n",
        "\n",
        "def table_history(tbl):\n",
        "  interesting_fields = \"timestamp, operation, operationmetrics,operationParameters\".split(',')\n",
        "  return spark.sql(f\"\"\"describe history {tbl}\"\"\").selectExpr(interesting_fields)\n",
        "\n",
        "IPython.display.HTML(\n",
        "    df_to_html(table_history('spark_streaming_bronze')      ,title='source') +\n",
        "    df_to_html(table_history('spark_streaming_silver_naive'),title='silver_1') +\n",
        "    df_to_html(table_history('spark_streaming_bronze')      ,title='source')\n",
        ")\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-0ddb69124e61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m IPython.display.HTML(\n\u001b[1;32m     32\u001b[0m     \u001b[0mdf_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark_streaming_bronze'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdf_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark_streaming_silver_naive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'silver_1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mdf_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark_streaming_bronze'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;34m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-92-0ddb69124e61>\u001b[0m in \u001b[0;36mtable_history\u001b[0;34m(tbl)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtable_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0minteresting_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"timestamp, operation, operationmetrics,operationParameters\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\"\"describe history {tbl}\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteresting_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m IPython.display.HTML(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: DESCRIBE HISTORY is only supported for Delta tables."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BYpVr0REM2t"
      },
      "source": [
        "## Reddit questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG5opCG2EP5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6efcf1-fdd6-496e-b74f-55575b5a56d3"
      },
      "source": [
        "# https://old.reddit.com/r/apachespark/comments/qrh5qn/join_3k_rows_with_4m_rows_to_create_3bn_row_delta/\n",
        "import time\n",
        "if True:\n",
        "  small_tbl_size = 3000\n",
        "  large_tbl_size = 4_000_000\n",
        "  join_col_cardinality = 4\n",
        "\n",
        "  small_data = [(x,f\"row {x}\", x%join_col_cardinality) for x in range(small_tbl_size)]\n",
        "  large_data = [(x,f\"row {x}\", x%join_col_cardinality) for x in range(large_tbl_size)]\n",
        "\n",
        "  small_df = spark.createDataFrame(small_data, 'id int, txt string, join_col int')\n",
        "  large_df = spark.createDataFrame(large_data, 'id int, txt string, join_col int')\n",
        "\n",
        "  small_df.createOrReplaceTempView('small_tbl')\n",
        "  large_df.createOrReplaceTempView('large_tbl')\n",
        "\n",
        "  t0 = time.time()\n",
        "  spark.sql(\"\"\"\n",
        "    SELECT * \n",
        "      FROM small_tbl AS s\n",
        "      JOIN large_tbl AS l ON (s.join_col = l.join_col)\n",
        "  \"\"\").show()\n",
        "  print(f\"{time.time() - t0} seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+--------+---+------+--------+\n",
            "| id|  txt|join_col| id|   txt|join_col|\n",
            "+---+-----+--------+---+------+--------+\n",
            "|  1|row 1|       1|  1| row 1|       1|\n",
            "|  1|row 1|       1|  5| row 5|       1|\n",
            "|  1|row 1|       1|  9| row 9|       1|\n",
            "|  1|row 1|       1| 13|row 13|       1|\n",
            "|  1|row 1|       1| 17|row 17|       1|\n",
            "|  1|row 1|       1| 21|row 21|       1|\n",
            "|  1|row 1|       1| 25|row 25|       1|\n",
            "|  1|row 1|       1| 29|row 29|       1|\n",
            "|  1|row 1|       1| 33|row 33|       1|\n",
            "|  1|row 1|       1| 37|row 37|       1|\n",
            "|  1|row 1|       1| 41|row 41|       1|\n",
            "|  1|row 1|       1| 45|row 45|       1|\n",
            "|  1|row 1|       1| 49|row 49|       1|\n",
            "|  1|row 1|       1| 53|row 53|       1|\n",
            "|  1|row 1|       1| 57|row 57|       1|\n",
            "|  1|row 1|       1| 61|row 61|       1|\n",
            "|  1|row 1|       1| 65|row 65|       1|\n",
            "|  1|row 1|       1| 69|row 69|       1|\n",
            "|  1|row 1|       1| 73|row 73|       1|\n",
            "|  1|row 1|       1| 77|row 77|       1|\n",
            "+---+-----+--------+---+------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "11.074525594711304 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOAV1lw7H6BX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7f4bf5-6a22-47d7-e797-a35b7e79ae6a"
      },
      "source": [
        "# https://www.reddit.com/r/apachespark/comments/r0fwrx/merge_two_rdds/\n",
        "d1 = [3,5,8]\n",
        "d2 = [1,2,3,4]\n",
        "df1 = spark.createDataFrame(d1,'int').createOrReplaceTempView('v1')\n",
        "df2 = spark.createDataFrame(d2,'int').createOrReplaceTempView('v2')\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "   select flatten(array(array(v2.value),v1s.values))\n",
        "     from v2 \n",
        "     join (select collect_list(value) as values from v1) as v1s\n",
        "\"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+\n",
            "|flatten(array(array(value), values))|\n",
            "+------------------------------------+\n",
            "|                        [1, 3, 5, 8]|\n",
            "|                        [2, 3, 5, 8]|\n",
            "|                        [3, 3, 5, 8]|\n",
            "|                        [4, 3, 5, 8]|\n",
            "+------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}