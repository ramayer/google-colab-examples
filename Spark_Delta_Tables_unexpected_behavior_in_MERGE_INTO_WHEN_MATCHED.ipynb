{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Spark delta.io Unexpected behavior in MERGE INTO ... WHEN MATCHED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramayer/google-colab-examples/blob/main/Spark_delta_io_Unexpected_behavior_in_MERGE_INTO_WHEN_MATCHED.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deH9mcRtiz4C"
      },
      "source": [
        "# Spark's Delta Table's MERGE INTO statement appears to unnecessarily overwrite parquet files.\n",
        "\n",
        "* Delta.io's \"MERGE INTO\" statement can sometimes re-write hundreds of files even though it successfully notices that only a single row changed.\n",
        "* Expected behaviour - a MERGE INTO statement that only touches one row should modify at most 3 files in a delta table (the parquet file that contains that row is added; the previous parquet file that contained the row is removed; and the transaction log is updated) \n",
        "* Observed behaviour - In the example below, even though only a single row is updated, hundreds of files are rewritten.\n"
      ],
      "id": "deH9mcRtiz4C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDySui-QjgYD"
      },
      "source": [
        "## Install & initialize Spark and its dependencies"
      ],
      "id": "qDySui-QjgYD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxvV1waFjgz8",
        "outputId": "2b17be93-e616-4c12-f763-30c4b6400215"
      },
      "source": [
        "!apt-get -qq install -y openjdk-8-jdk-headless > /tmp/apt-get.out\n",
        "!(wget -q --show-progress -nc https://mirrors.ocf.berkeley.edu/apache/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz)\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "try:\n",
        "  import pyspark, findspark, delta\n",
        "except:\n",
        "  %pip install -q --upgrade pyspark findspark delta\n"
      ],
      "id": "nxvV1waFjgz8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.1.2-bin-had 100%[===================>] 218.23M  19.0MB/s    in 13s     \n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 36 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.3 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for delta (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "regional-dynamics",
        "outputId": "94213655-fe22-444f-906b-69818001b36b"
      },
      "source": [
        "import findspark\n",
        "import pyspark\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "MAX_MEMORY=\"8g\"\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (pyspark.sql.SparkSession.builder.appName(\"MyApp\") \n",
        "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
        "    .config(\"spark.executor.memory\", MAX_MEMORY) \n",
        "    .config(\"spark.driver.memory\", MAX_MEMORY) \n",
        "    .enableHiveSupport() \n",
        "    .getOrCreate()        \n",
        "    )\n",
        "\n",
        "spark"
      ],
      "id": "regional-dynamics",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e5d9de7a236a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MyApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7d59263e50>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2KJpd_jlt5K"
      },
      "source": [
        "## Create Test Delta Tables\n",
        "\n",
        "* one for the merge source, and \n",
        "* one for the merge destination"
      ],
      "id": "r2KJpd_jlt5K"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fitting-token"
      },
      "source": [
        "src_df = spark.range(1000   ).selectExpr(\"id\",\"'data for ' || id as data\")\n",
        "dst_df = spark.range(100_000).selectExpr(\"id\",\"'data for ' || id as data\")\n",
        "\n",
        "src_df.write.format('delta').mode('overwrite').saveAsTable(\"tmp_merge_src\")\n",
        "dst_df.write.format('delta').mode('overwrite').saveAsTable(\"tmp_merge_dst\")\n"
      ],
      "id": "fitting-token",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "built-beast",
        "outputId": "0e58ba9b-55b9-419c-ccb3-0ff46744f853"
      },
      "source": [
        "print(\"Merge Source\")\n",
        "src_df.show(3)\n",
        "print(\"Merge Destination\")\n",
        "dst_df.show(3)"
      ],
      "id": "built-beast",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge Source\n",
            "+---+----------+\n",
            "| id|      data|\n",
            "+---+----------+\n",
            "|  0|data for 0|\n",
            "|  1|data for 1|\n",
            "|  2|data for 2|\n",
            "+---+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Merge Destination\n",
            "+---+----------+\n",
            "| id|      data|\n",
            "+---+----------+\n",
            "|  0|data for 0|\n",
            "|  1|data for 1|\n",
            "|  2|data for 2|\n",
            "+---+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surface-latvia"
      },
      "source": [
        "# debug function to show table history\n",
        "import json\n",
        "import pyspark.sql.functions as psf\n",
        "def show_history():\n",
        "    hist = spark.sql(\"describe history tmp_merge_dst\").selectExpr(\n",
        "       \"timestamp\",\"operation\",\"operationmetrics\", \"operationparameters\", \n",
        "    ).sort(psf.col('timestamp').desc())\n",
        "    print(json.dumps(hist.take(1),default=str,indent=1))"
      ],
      "id": "surface-latvia",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4KSg_LunFSZ"
      },
      "source": [
        "## This MERGE INTO statement will only update a single row, but unnecessariy overwrite over 100 parquet files.\n",
        "\n",
        "* As the history shows below, it correctly noticed that only a single row was modified.   However it rewrites hundreds of files.\n",
        "* It should have noticed that most of those files were unaffected because of the \"AND s.data <> d.data\" clause.\n"
      ],
      "id": "i4KSg_LunFSZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "worst-reference",
        "outputId": "cc6c6c98-a8b5-4e5b-8854-593e22dade2f"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "  UPDATE tmp_merge_src SET data='only this row should be updated, but it seems many are rewritten' where id = 1\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "MERGE INTO tmp_merge_dst as d\n",
        "     USING tmp_merge_src as s\n",
        "     ON (s.id = d.id)\n",
        "     WHEN MATCHED AND s.data <> d.data THEN UPDATE SET *\n",
        "     WHEN NOT MATCHED THEN INSERT *\n",
        "\"\"\")\n",
        "show_history()"
      ],
      "id": "worst-reference",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            " [\n",
            "  \"2021-11-19 05:35:49\",\n",
            "  \"MERGE\",\n",
            "  {\n",
            "   \"numOutputRows\": \"50000\",\n",
            "   \"numTargetRowsInserted\": \"0\",\n",
            "   \"numTargetRowsUpdated\": \"1\",\n",
            "   \"numTargetFilesAdded\": \"200\",\n",
            "   \"numTargetFilesRemoved\": \"1\",\n",
            "   \"numTargetRowsDeleted\": \"0\",\n",
            "   \"scanTimeMs\": \"4682\",\n",
            "   \"numSourceRows\": \"1000\",\n",
            "   \"executionTimeMs\": \"16412\",\n",
            "   \"numTargetRowsCopied\": \"49999\",\n",
            "   \"rewriteTimeMs\": \"11724\"\n",
            "  },\n",
            "  {\n",
            "   \"matchedPredicates\": \"[{\\\"predicate\\\":\\\"(NOT (s.`data` = d.`data`))\\\",\\\"actionType\\\":\\\"update\\\"}]\",\n",
            "   \"predicate\": \"(s.`id` = d.`id`)\",\n",
            "   \"notMatchedPredicates\": \"[{\\\"actionType\\\":\\\"insert\\\"}]\"\n",
            "  }\n",
            " ]\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1jEU3JcsatQ"
      },
      "source": [
        "## This workaround produces the expected results.\n",
        "\n",
        "* By filtering in the USING clause instead of the WHEN MATCHED clause, it correctly only modifies 1 file (and the transaction log)\n",
        "* Ideally the filter in the WHEN MATCHED clause would do the same thing.\n"
      ],
      "id": "j1jEU3JcsatQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "automotive-acrobat",
        "outputId": "16dd3e02-b699-48cb-bce6-9af973d315c6"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "  UPDATE tmp_merge_src SET data='only this row should be updated' where id = 1\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "MERGE INTO tmp_merge_dst as d\n",
        "     USING (select * from tmp_merge_src as n where not exists (select 1 from tmp_merge_dst as o where n.id=o.id and n.data = o.data)) as s\n",
        "     ON (s.id = d.id)\n",
        "     WHEN MATCHED AND s.data <> d.data THEN UPDATE SET *\n",
        "     WHEN NOT MATCHED THEN INSERT *\n",
        "\"\"\")\n",
        "show_history()"
      ],
      "id": "automotive-acrobat",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            " [\n",
            "  \"2021-11-18 03:07:56.899000\",\n",
            "  \"MERGE\",\n",
            "  {\n",
            "   \"numOutputRows\": \"39\",\n",
            "   \"numTargetRowsInserted\": \"0\",\n",
            "   \"numTargetRowsUpdated\": \"1\",\n",
            "   \"numTargetFilesAdded\": \"2\",\n",
            "   \"numTargetFilesRemoved\": \"1\",\n",
            "   \"numTargetRowsDeleted\": \"0\",\n",
            "   \"scanTimeMs\": \"874\",\n",
            "   \"numSourceRows\": \"1\",\n",
            "   \"executionTimeMs\": \"1749\",\n",
            "   \"numTargetRowsCopied\": \"38\",\n",
            "   \"rewriteTimeMs\": \"873\"\n",
            "  },\n",
            "  {\n",
            "   \"matchedPredicates\": \"[{\\\"predicate\\\":\\\"(NOT (s.`data` = d.`data`))\\\",\\\"actionType\\\":\\\"update\\\"}]\",\n",
            "   \"predicate\": \"(s.`id` = d.`id`)\",\n",
            "   \"notMatchedPredicates\": \"[{\\\"actionType\\\":\\\"insert\\\"}]\"\n",
            "  }\n",
            " ]\n",
            "]\n"
          ]
        }
      ]
    }
  ]
}